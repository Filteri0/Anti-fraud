{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "UyiNa76npL62"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\Venv\\Pytorch_venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from ckip_transformers.nlp import CkipWordSegmenter, CkipPosTagger, CkipNerChunker\n",
        "import pandas as pd\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "df_0 資料筆數: 1518\n",
            "df_0 原始內文: 不要輕易向陌生人透露個人金融信息，以防被騙。\n",
            "=========================================================\n",
            "df_1 資料筆數: 1814\n",
            "df_1 原始內文: 配合完成任一方案即可領取福利加碼\n",
            "限定名額100名\n",
            "https://line.me/ti/p/QxL551-7u0\n"
          ]
        }
      ],
      "source": [
        "ws_driver = CkipWordSegmenter(model=\"bert-base\", device=0)\n",
        "pos_driver = CkipPosTagger(model=\"bert-base\", device=0)\n",
        "\n",
        "df_0 = pd.read_csv(\"./data/data_0.csv\")\n",
        "df_1 = pd.read_csv(\"./data/data_1.csv\")\n",
        "\n",
        "print(f\"df_0 資料筆數: {len(df_0)}\")\n",
        "print(f\"df_0 原始內文: {df_0['內文'][10]}\")\n",
        "print(\"=========================================================\")\n",
        "print(f\"df_1 資料筆數: {len(df_1)}\")\n",
        "print(f\"df_1 原始內文: {df_1['內文'][10]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def remove_special_characters(text):\n",
        "    # 保留中文字、英文字母、數字以及常見標點符號\n",
        "    pattern = r'[^a-zA-Z0-9\\u4e00-\\u9fff，。！？、；：‘’“”（）《》〈〉【】{}]'\n",
        "    return re.sub(pattern, '', text)\n",
        "\n",
        "# 定義過濾詞性的函數\n",
        "def clean(sentence_ws, sentence_pos):\n",
        "    short_sentence = []\n",
        "    stop_pos = set(['Nep', 'Nh'])\n",
        "    for word_ws, word_pos in zip(sentence_ws, sentence_pos):\n",
        "        # 去掉名詞裡的某些詞性\n",
        "        is_not_stop_pos = word_pos not in stop_pos\n",
        "        # 組成串列\n",
        "        if is_not_stop_pos:\n",
        "            short_sentence.append(word_ws)\n",
        "    return \"\".join(short_sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenization: 100%|██████████| 1518/1518 [00:00<00:00, 84375.21it/s]\n",
            "Inference:   0%|          | 0/6 [00:00<?, ?it/s]d:\\Venv\\Pytorch_venv\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:435: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
            "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
            "Inference: 100%|██████████| 6/6 [00:15<00:00,  2.61s/it]\n",
            "Tokenization: 100%|██████████| 1518/1518 [00:00<00:00, 108436.43it/s]\n",
            "Inference: 100%|██████████| 10/10 [00:14<00:00,  1.42s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "清理後內文: 不要輕易向陌生人透露個人金融信息，以防被騙。\n",
            "斷詞結果: 不要輕易向陌生人透露金融信息，以防被騙。\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tokenization: 100%|██████████| 1814/1814 [00:00<00:00, 7092.52it/s]\n",
            "Inference: 100%|██████████| 8/8 [27:59<00:00, 209.99s/it]\n",
            "Tokenization: 100%|██████████| 1814/1814 [00:00<00:00, 4047.40it/s]\n",
            "Inference: 100%|██████████| 58/58 [1:16:07<00:00, 78.75s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "清理後內文: 配合完成任一方案即可領取福利加碼限定名額100名httpslinemetipQxL5517u0\n",
            "斷詞結果: 配合完成任一方案即可領取福利加碼限定名額100名httpslinemetipQxL5517u0\n"
          ]
        }
      ],
      "source": [
        "df_0['內文_清理後'] = df_0['內文'].apply(remove_special_characters) # 清理資料\n",
        "\n",
        "# 進行斷詞和詞性標註\n",
        "text_list = df_0['內文_清理後'].tolist()\n",
        "ws_result = ws_driver(text_list)\n",
        "pos_result = pos_driver(ws_result)\n",
        "\n",
        "# 將斷詞和詞性標註結果進行過濾\n",
        "filtered_sentences = [clean(sentence_ws, sentence_pos) for sentence_ws, sentence_pos in zip(ws_result, pos_result)]\n",
        "\n",
        "# 將過濾後的結果存儲回 DataFrame\n",
        "df_0['內文_斷詞後'] = filtered_sentences\n",
        "\n",
        "# 顯示前處理後的內文\n",
        "print(f\"清理後內文: {df_0['內文_清理後'][10]}\")\n",
        "print(f\"斷詞結果: {df_0['內文_斷詞後'][10]}\")\n",
        "\n",
        "df_0.to_csv(\"./train_data/data_0.csv\", index=False, encoding=\"utf-8-sig\")\n",
        "\n",
        "df_1['內文_清理後'] = df_1['內文'].apply(remove_special_characters) # 清理資料\n",
        "\n",
        "# 進行斷詞和詞性標註\n",
        "text_list = df_1['內文_清理後'].tolist()\n",
        "ws_result = ws_driver(text_list)\n",
        "pos_result = pos_driver(ws_result)\n",
        "\n",
        "# 將斷詞和詞性標註結果進行過濾\n",
        "filtered_sentences = [clean(sentence_ws, sentence_pos) for sentence_ws, sentence_pos in zip(ws_result, pos_result)]\n",
        "\n",
        "# 將過濾後的結果存儲回 DataFrame\n",
        "df_1['內文_斷詞後'] = filtered_sentences\n",
        "\n",
        "# 顯示前處理後的內文\n",
        "print(f\"清理後內文: {df_1['內文_清理後'][10]}\")\n",
        "print(f\"斷詞結果: {df_1['內文_斷詞後'][10]}\")\n",
        "\n",
        "df_1.to_csv(\"./train_data/data_1.csv\", index=False, encoding=\"utf-8-sig\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Pytorch_venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
